{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funcitons for wine_tasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\annat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\annat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\annat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\annat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\annat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\annat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\annat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn_recommender as skr\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frequency_of_occurence(df, col):\n",
    "    \n",
    "    s2=df[col]\n",
    "    prob = s2.value_counts(normalize=True)\n",
    "    threshold = 0.01\n",
    "    mask = prob > threshold\n",
    "    tail_prob = prob.loc[~mask].sum()\n",
    "    prob = prob.loc[mask]\n",
    "    prob['other'] = tail_prob\n",
    "    prob = prob.to_frame()\n",
    "    prob = prob.rename(columns={col: \"frequency\"}) \n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title('Showing % of occurence in '+col)\n",
    "    sns.barplot(y=prob.index, x='frequency', data = prob, palette=\"Reds_d\");\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_decription(descr_text):\n",
    "    # split into words\n",
    "    tokens = word_tokenize(descr_text)\n",
    "\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update([\"drink\", \"now\", \"wine\", \"flavor\", \"flavors\"])\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    #print(words[:100])\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "\n",
    "    #print(stemmed[:100])\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq_words(df, col, how_many = 10, title = None, size = (10,10)):\n",
    "    \n",
    "    l = list(df[col])\n",
    "    flat_list = [item for sublist in l for item in sublist]\n",
    "    freq = FreqDist(flat_list)\n",
    "\n",
    "    most_frequent_words = pd.DataFrame(freq.most_common(how_many), columns =['word', 'freq'])\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=size)\n",
    "    plt.title(title)\n",
    "    sns.barplot(x=\"freq\", y=\"word\", data=most_frequent_words, palette=\"Blues_d\");\n",
    "\n",
    "    return most_frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_words(querywords):\n",
    "    unwanted_words = ['wine']\n",
    "    return [word for word in querywords if word not in unwanted_words]\n",
    "\n",
    "#wine_data['parsed_descr']  = wine_data['parsed_descr'].apply(rem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimentionality(X, n_components = 3):\n",
    "    \n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_from_dataframe(df, number_of_clusters = 1):\n",
    "    list_of_clusters = list()\n",
    "    \n",
    "    for i in range(number_of_clusters):\n",
    "            #print(i)\n",
    "            cluster = df.loc[df['cluster'] == i] \n",
    "            cluster = cluster.reset_index()\n",
    "            cols = [col for col in cluster.columns if not(col.startswith('Unnamed'))]\n",
    "            cluster=cluster[cols]\n",
    "            cluster = cluster.rename(columns={\"index\": \"original_index\"})\n",
    "                               \n",
    "            list_of_clusters.append(cluster)\n",
    "\n",
    "    print('I created '+ str(len(list_of_clusters)) + ' clusters.') \n",
    "    return list_of_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_word_cloud(text):\n",
    "\n",
    "    wordcloud = WordCloud(max_words=50, background_color=\"white\").generate(text)\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_from_title(search_title, search_variety, df, clusters_list):\n",
    "    \n",
    "    #df.query('A > B')\n",
    "    df = df[df.title.str.contains(search_title)]\n",
    "\n",
    "    idx_in_df = df[df.variety.str.contains(search_variety)].index.values[0]\n",
    "    which_cluster = df.loc[idx_in_df, 'cluster']\n",
    "    c = clusters_list[which_cluster]\n",
    "    \n",
    "    idx =c[c.original_index == idx_in_df].index.values[0]\n",
    "    #print(idx)\n",
    "    \n",
    "    return (idx_in_df, which_cluster, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variety_in_cluster(cluster, n = 20, title = None):\n",
    "\n",
    "    variety_counts = (cluster['variety'].value_counts()[:n])\n",
    "    variety_counts = variety_counts.to_frame()\n",
    "    variety_counts = variety_counts.rename(columns={\"variety\": \"frequency\"}) \n",
    "    #display(variety_counts)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(title)\n",
    "    sns.barplot(y=variety_counts.index, x='frequency', data = variety_counts, palette=\"Greens_d\");\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def find_unique_freq_words(clusters, how_many_words):\n",
    "    # words that are found in both sets\n",
    "    print('cluster 0' )\n",
    "    rem = (list(set(most_frequent_words[0].word) & set(most_frequent_words[1].word)))\n",
    "\n",
    "    # using list comprehension to perform task \n",
    "    res = [i for i in most_frequent_words[0].word if i not in rem] \n",
    "\n",
    "    rem = (list(set(res) & set(most_frequent_words[2].word)))\n",
    "\n",
    "    res = [i for i in res if i not in rem] \n",
    "\n",
    "    rem = (list(set(res) & set(most_frequent_words[3].word)))\n",
    "\n",
    "    res = [i for i in res if i not in rem] \n",
    "\n",
    "\n",
    "    res = [i for i in res if i not in rem] \n",
    "    res\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "l = list(cluster_0['parsed_descr'])\n",
    "flat_list = [item for sublist in l for item in sublist]\n",
    "freq = FreqDist(flat_list)\n",
    "\n",
    "most_frequent_words.append(pd.DataFrame(freq.most_common(20), columns =['word', 'freq']))\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=\"freq\", y=\"word\", data=most_frequent_words[0], palette=\"Blues\");\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf = reduce_dimentionality(tfidf_model, 400)\n",
    "\n",
    "#crds =  TSNE(n_components = 3).fit_transform(tfidf)\n",
    "\n",
    "#tsne_df = pd.DataFrame(crds, columns=['x','y','z'])\n",
    "\"\"\"\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(tsne_df['x'], tsne_df['y'], tsne_df['z'], c='skyblue', s=60)\n",
    "#ax.view_init(30, 185)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
